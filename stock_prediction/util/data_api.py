import akshare as ak
import pandas as pd
from datetime import datetime
import os
import time
import random
import functools
from typing import Optional, List
import re
import shutil


root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
path = os.path.join(root_dir, 'data')

def timer(func):
    """
    è®¡æ—¶è£…é¥°å™¨ï¼Œç”¨äºç»Ÿè®¡å‡½æ•°æ‰§è¡Œæ—¶é—´
    
    Args:
        func: è¦è®¡æ—¶çš„å‡½æ•°
    
    Returns:
        åŒ…è£…åçš„å‡½æ•°
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        duration = end_time - start_time
        print(f"{func.__name__} æ‰§è¡Œè€—æ—¶: {duration:.2f}ç§’")
        return result
    return wrapper

def sleep_random(seconds: float = 1.0, random_range: float = 0.5) -> None:
    """
    éšæœºå»¶æ—¶å‡½æ•°ï¼Œç”¨äºé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
    
    Args:
        seconds: åŸºç¡€å»¶æ—¶ç§’æ•°
        random_range: éšæœºæµ®åŠ¨èŒƒå›´ï¼ˆç§’ï¼‰
    """
    time.sleep(seconds + random.uniform(0, random_range))

def create_path(path_str: str) -> None:
    """
    é€’å½’åˆ›å»ºè·¯å¾„ï¼Œå¦‚æœè·¯å¾„ä¸å­˜åœ¨åˆ™åˆ›å»º
    
    Args:
        path_str: è¦åˆ›å»ºçš„è·¯å¾„å­—ç¬¦ä¸²
    """
    if not os.path.exists(path_str):
        os.makedirs(path_str)
        print(f"åˆ›å»ºè·¯å¾„: {path_str}")

def check_file_exists(file_path: str, max_age_days: int = 30) -> bool:
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        max_age_days: æ–‡ä»¶æœ€å¤§æœ‰æ•ˆæœŸï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤30å¤©
    
    Returns:
        bool: æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ
    """
    if not os.path.exists(file_path):
        return False
    
    # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
    file_mtime = os.path.getmtime(file_path)
    current_time = time.time()
    age_days = (current_time - file_mtime) / (24 * 3600)
    
    if age_days > max_age_days:
        print(f"æ–‡ä»¶å·²è¿‡æœŸï¼ˆ{age_days:.1f}å¤©ï¼‰ï¼Œéœ€è¦æ›´æ–°: {file_path}")
        return False
    
    return True

@timer
def fetch_with_cache(func, file_path: str, max_age_days: int = 30, position: str = None) -> bool:
    """
    å¸¦ç¼“å­˜æ£€æŸ¥çš„æ•°æ®è·å–å‡½æ•°
    
    Args:
        func: è·å–æ•°æ®çš„å‡½æ•°
        file_path: æ–‡ä»¶ä¿å­˜è·¯å¾„
        max_age_days: æ–‡ä»¶æœ€å¤§æœ‰æ•ˆæœŸï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤30å¤©
        position: è·å–ä½ç½®ï¼Œ'first' è¡¨ç¤ºç¬¬ä¸€è¡Œï¼Œ'last' è¡¨ç¤ºæœ€åä¸€è¡Œ, None è¡¨ç¤ºå…¨éƒ¨
    """
    if check_file_exists(file_path, max_age_days):
        print(f"âœ… æ–‡ä»¶æœ‰æ•ˆï¼Œè·³è¿‡è·å–: {file_path}")
        return True
    
    if position is not None and position != 'first' and position != 'last':
        print(f"âŒ ä½ç½®å‚æ•°é”™è¯¯: {position}")
        return False
    
    print(f"ğŸ” è·å–æ–°æ•°æ®: {file_path}")
    try:
        sleep_random(1.5, 0.5)  # æ·»åŠ éšæœºå»¶æ—¶
        df = func()
        print('df: ', df)
        if position == 'first':
            df = df.iloc[0]
        elif position == 'last':
            df = df.iloc[-1]
        df.to_csv(file_path, index=False, encoding='utf-8-sig')
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
        return True
    except Exception as e:
        print(f"âŒ è·å–æ•°æ®å¤±è´¥: {str(e)}")
        sleep_random(3.0, 1.0)  # å¤±è´¥åç­‰å¾…æ›´é•¿æ—¶é—´
        try:
            df = func()
            if position == 'first':
                df = df.iloc[0]
            elif position == 'last':
                df = df.iloc[-1]
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            print(f"âœ… é‡è¯•æˆåŠŸï¼šæ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
            return True
        except Exception as e2:
            print(f"âŒ é‡è¯•å¤±è´¥: {str(e2)}")
            return False


@timer
def fetch_stock_daily_data(stock_code: int, start_date: str, end_date: str) -> None:
    """
    è·å–è‚¡ç¥¨æ—¥çº¿æ•°æ®
    """
    global path
    daily_data_path = os.path.join(path, str(stock_code), 'è‚¡ç¥¨æ—¥çº¿æ•°æ®.csv')

    # è·å–è‚¡ç¥¨æ—¥çº¿æ•°æ®
    print('ğŸ” æ­£åœ¨è·å–è‚¡ç¥¨æ—¥çº¿æ•°æ®...')
    try:
        stock_df = ak.stock_zh_a_hist(symbol=stock_code, period="daily", 
                                      start_date=start_date, end_date=end_date,
                                      adjust="qfq")
        # åˆ é™¤ä¸éœ€è¦çš„åˆ—
        stock_df = stock_df.drop('è‚¡ç¥¨ä»£ç ', axis=1)
        # ä¿å­˜æ—¥çº¿æ•°æ®
        stock_df.to_csv(daily_data_path, index=False, encoding='utf-8-sig')
        print(f'âœ… è‚¡ç¥¨æ—¥çº¿æ•°æ®å·²ä¿å­˜åˆ° {daily_data_path}')
    except Exception as e:
        print(f"âŒ è·å–è‚¡ç¥¨æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}")

@timer
def fetch_stock_news_data(stock_code: int) -> None:
    """
    è·å–è‚¡ç¥¨æ–°é—»æ•°æ®
    """
    global path
    news_data_path = os.path.join(path, str(stock_code), 'è‚¡ç¥¨æ–°é—»æ•°æ®.csv')

    # è·å–æ–°é—»æ•°æ®
    print('ğŸ” æ­£åœ¨è·å–æ–°é—»æ•°æ®...')
    try:
        news_df = ak.stock_news_em(symbol=stock_code)
        # åˆ é™¤ä¸éœ€è¦çš„åˆ—
        news_df = news_df.drop('å…³é”®è¯', axis=1)
        # æŒ‰æ—¶é—´æ’åº
        news_df['å‘å¸ƒæ—¶é—´'] = pd.to_datetime(news_df['å‘å¸ƒæ—¶é—´'], format='%Y-%m-%d %H:%M:%S')
        news_df = news_df.sort_values('å‘å¸ƒæ—¶é—´', ascending=False)
        news_df.to_csv(news_data_path, index=False, encoding='utf-8-sig')
        print(f"âœ… æ–°é—»æ•°æ®å·²ä¿å­˜åˆ° {news_data_path}")
    except Exception as e:
        print(f"âŒ è·å–æ–°é—»æ•°æ®å¤±è´¥: {str(e)}")

@timer
def fetch_stock_fundamentals_data(stock_code: int):
    """
    è·å–è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®
    """
    global path
    fundamentals_data_path = os.path.join(path, str(stock_code), 'è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®')
    
    # è·å–ä¸ªè‚¡ç ”æŠ¥
    fetch_stock_report_data(stock_code, fundamentals_data_path)

    # è·å–ä¸šç»©æŠ¥è¡¨
    fetch_stock_yjbb_data(stock_code, fundamentals_data_path)

    # è·å–ä¸»è¥ä»‹ç»
    fetch_stock_zyjs_data(stock_code, fundamentals_data_path)

    # è·å–ä¸»è¥æ„æˆ
    fetch_stock_zygc_data(stock_code, fundamentals_data_path)

    # è·å–åŸºæœ¬é¢æ•°æ®å…³é”®æŒ‡æ ‡
    fetch_stock_financial_abstract_data(stock_code, fundamentals_data_path)


@timer
def fetch_stock_report_data(stock_code: int, fundamentals_data_path: str) -> None:
    """
    è·å–ä¸ªè‚¡ç ”æŠ¥
    """
    create_path(fundamentals_data_path)
    report_data_path = os.path.join(fundamentals_data_path, 'ä¸ªè‚¡ç ”æŠ¥.csv')
    
    # è·å–ä¸ªè‚¡ç ”æŠ¥
    print('ğŸ” æ­£åœ¨è·å–ä¸ªè‚¡ç ”æŠ¥...')
    fetch_with_cache(lambda: ak.stock_research_report_em(symbol=stock_code), report_data_path)


# è·å–æœ€è¿‘çš„å­£åº¦æ—¥æœŸ
def get_latest_quarter_date(current_date: datetime, current_year: int) -> str:
    """
    è·å–æœ€è¿‘çš„å­£åº¦æ—¥æœŸ
    """
    quarter_dates = [
        f"{current_year}0331",  # ç¬¬ä¸€å­£åº¦
        f"{current_year}0630",  # ç¬¬äºŒå­£åº¦
        f"{current_year}0930",  # ç¬¬ä¸‰å­£åº¦
        f"{current_year}1231"   # ç¬¬å››å­£åº¦
    ]
    latest_date = None
    for date in quarter_dates:
        date_obj = datetime.strptime(date, "%Y%m%d")
        if date_obj < current_date:
            latest_date = date
        else:
            break
    if latest_date is None:
        current_year -= 1
        return get_latest_quarter_date(current_date, current_year)
    return latest_date


# è·å–æœ€è¿‘çš„ä¸¤ä¸ªå­£åº¦æ—¥æœŸ
def get_latest_two_quarter_dates(current_date: datetime, current_year: int) -> List[str]:
    """
    è·å–æœ€è¿‘çš„ä¸¤ä¸ªå­£åº¦æ—¥æœŸï¼ˆæŒ‰æ—¶é—´é¡ºåºæ’åˆ—ï¼‰
    ä¾‹å¦‚ï¼šå½“å‰æ—¥æœŸæ˜¯2023-08-15ï¼Œåˆ™è¿”å› ['20230630', '20230331']
    """
    # ç”Ÿæˆå½“å‰å¹´å’Œå‰ä¸€å¹´çš„æ‰€æœ‰å­£åº¦æ—¥æœŸ
    quarter_dates = []
    for year in [current_year - 1, current_year]:
        quarter_dates.extend([
            f"{year}0331",  # ç¬¬ä¸€å­£åº¦
            f"{year}0630",  # ç¬¬äºŒå­£åº¦
            f"{year}0930",  # ç¬¬ä¸‰å­£åº¦
            f"{year}1231"   # ç¬¬å››å­£åº¦
        ])
    
    # è½¬æ¢ä¸ºdatetimeå¯¹è±¡å¹¶è¿‡æ»¤æ‰æœªæ¥æ—¥æœŸ
    valid_dates = []
    for date_str in quarter_dates:
        date_obj = datetime.strptime(date_str, "%Y%m%d")
        if date_obj < current_date:
            valid_dates.append(date_obj)
    
    # æŒ‰æ—¥æœŸé™åºæ’åºå¹¶å–æœ€è¿‘çš„ä¸¤ä¸ª
    valid_dates.sort(reverse=True)
    latest_two = valid_dates[:2]
    
    # è½¬æ¢å›å­—ç¬¦ä¸²æ ¼å¼å¹¶æŒ‰æ—¶é—´é¡ºåºæ’åˆ—ï¼ˆæ—©çš„åœ¨å‰ï¼‰
    return [d.strftime("%Y%m%d") for d in sorted(latest_two)]



@timer
def fetch_stock_yjbb_data(stock_code: int, fundamentals_data_path: str, target_date: str = None) -> None:
    """
    è·å–ä¸šç»©æŠ¥è¡¨
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç 
        fundamentals_data_path: åŸºæœ¬é¢æ•°æ®ä¿å­˜è·¯å¾„
    """
    create_path(fundamentals_data_path)
    yjbb_data_path = os.path.join(fundamentals_data_path, 'ä¸šç»©æŠ¥è¡¨.csv')
    
    current_date = None
    current_year = None

    if target_date is None:
        # è·å–å½“å‰æ—¥æœŸ
        current_date = datetime.now()
        current_year = current_date.year
    else:
        current_date = datetime.strptime(target_date, '%Y-%m-%d')
        current_year = current_date.year

    # æ‰¾åˆ°æœ€è¿‘çš„å·²è¿‡æœŸçš„ä¸¤ä¸ªå­£åº¦æ—¥æœŸ
    latest_date = get_latest_two_quarter_dates(current_date, current_year)

    print(f"æœ€è¿‘çš„å·²è¿‡æœŸçš„ä¸¤ä¸ªå­£åº¦æ—¥æœŸ: {latest_date}")

    # è·å–ä¸šç»©æŠ¥è¡¨
    def fetch_and_filter_data(date_list: List[str]) -> pd.DataFrame:
        result_dfs = []
        for date in date_list:
            sleep_random()
            print(f'ğŸ” æ­£åœ¨è·å–ä¸šç»©æŠ¥è¡¨ï¼Œä½¿ç”¨æ—¥æœŸ: {date}...')
            # è·å–æ‰€æœ‰è‚¡ç¥¨çš„ä¸šç»©æŠ¥è¡¨
            try:
                df = ak.stock_yjbb_em(date=date)
            except Exception as e:
                print(f"âŒ è·å–æ•°æ®å¤±è´¥: {str(e)}")
                continue
            
            if df is None or df.empty:
                print(f"âŒ æ—¥æœŸ {date} æ²¡æœ‰ä¸šç»©æŠ¥è¡¨æ•°æ®æˆ–æ¥å£è¿”å›å¼‚å¸¸")
                continue

            print('å¼€å§‹ç­›é€‰')
            # ç­›é€‰æŒ‡å®šè‚¡ç¥¨çš„æ•°æ®
            stock_data = df[df['è‚¡ç¥¨ä»£ç '] == str(stock_code)]
            stock_data.insert(loc=0, column='æ—¶é—´', value=date)
            print('stock_data: ', stock_data)

            if stock_data.empty:
                print(f"âŒ æ—¥æœŸ {date} æ²¡æœ‰è‚¡ç¥¨ {stock_code} çš„ä¸šç»©æŠ¥è¡¨æ•°æ®")
                continue
            
            result_dfs.append(stock_data)
        
        return pd.concat(result_dfs).drop_duplicates()
    
    fetch_with_cache(lambda: fetch_and_filter_data(latest_date), yjbb_data_path)


@timer
def fetch_stock_zyjs_data(stock_code: int, fundamentals_data_path: str) -> None:
    """
    è·å–ä¸»è¥ä»‹ç»
    """
    create_path(fundamentals_data_path)
    zyjs_data_path = os.path.join(fundamentals_data_path, 'ä¸»è¥ä»‹ç».csv')
    
    # è·å–ä¸»è¥ä»‹ç»
    print('ğŸ” æ­£åœ¨è·å–ä¸»è¥ä»‹ç»...')
    fetch_with_cache(lambda: ak.stock_zyjs_ths(symbol=stock_code), zyjs_data_path)


@timer
def fetch_stock_zygc_data(stock_code: int, fundamentals_data_path: str) -> None:
    """
    è·å–ä¸»è¥æ„æˆ
    """
    create_path(fundamentals_data_path)
    zygc_data_path = os.path.join(fundamentals_data_path, 'ä¸»è¥æ„æˆ.csv')
    
    # è·å–ä¸»è¥æ„æˆ
    print('ğŸ” æ­£åœ¨è·å–ä¸»è¥æ„æˆ...')
    result = fetch_with_cache(lambda: ak.stock_zygc_em(symbol='SH' + str(stock_code)), zygc_data_path)
    
    # å¦‚æœæ²ªå¸‚è·å–å¤±è´¥ï¼Œåˆ™å°è¯•æ·±å¸‚
    if not result:
        fetch_with_cache(lambda: ak.stock_zygc_em(symbol='SZ' + str(stock_code)), zygc_data_path)


@timer
def fetch_stock_financial_abstract_data(stock_code: int, fundamentals_data_path: str) -> None:
    """
    è·å–åŸºæœ¬é¢æ•°æ®å…³é”®æŒ‡æ ‡
    """
    create_path(fundamentals_data_path)
    financial_abstract_data_path = os.path.join(fundamentals_data_path, 'åŸºæœ¬é¢æ•°æ®å…³é”®æŒ‡æ ‡.csv')

    # è·å–åŸºæœ¬é¢æ•°æ®å…³é”®æŒ‡æ ‡
    print('ğŸ” æ­£åœ¨è·å–åŸºæœ¬é¢æ•°æ®å…³é”®æŒ‡æ ‡...')
    fetch_with_cache(lambda: ak.stock_financial_abstract_ths(symbol=stock_code), financial_abstract_data_path)


@timer
def fetch_macro_data():
    """
    è·å–å®è§‚æ•°æ®
    """
    print('ğŸ” æ­£åœ¨è·å–å®è§‚æ•°æ®...')
    global path
    macro_data_path = os.path.join(path, 'å®è§‚æ•°æ®')
    
    fetch_macro_cn(os.path.join(macro_data_path, 'ä¸­å›½å®è§‚æ•°æ®'))
    fetch_macro_us(os.path.join(macro_data_path, 'ç¾å›½å®è§‚æ•°æ®'))


def fetch_macro_cn(macro_cn_data_path: str):
    """
    è·å–ä¸­å›½å®è§‚æ•°æ®
    """
    print('ğŸ” æ­£åœ¨è·å–ä¸­å›½å®è§‚æ•°æ®...')
    create_path(macro_cn_data_path)

    # è·å–ä¸­å›½å®è§‚æ æ†ç‡æ•°æ®
    fetch_macro_cnbs_data(os.path.join(macro_cn_data_path, 'å®è§‚æ æ†ç‡æ•°æ®.csv'))

    # è·å–å›½æ°‘ç»æµè¿è¡ŒçŠ¶å†µæ•°æ®
    fetch_macro_cn_gmjjy(os.path.join(macro_cn_data_path, 'å›½æ°‘ç»æµè¿è¡ŒçŠ¶å†µæ•°æ®'))

    # è·å–è´¸æ˜“çŠ¶å†µæ•°æ®
    fetch_macro_cn_myzk(os.path.join(macro_cn_data_path, 'è´¸æ˜“çŠ¶å†µæ•°æ®'))

    # è·å–äº§ä¸šæŒ‡æ ‡
    fetch_macro_cn_cyzb(os.path.join(macro_cn_data_path, 'äº§ä¸šæŒ‡æ ‡'))

    # è·å–é‡‘èæŒ‡æ ‡
    fetch_macro_cn_jrzb(os.path.join(macro_cn_data_path, 'é‡‘èæŒ‡æ ‡'))


def fetch_macro_cnbs_data(macro_cnbs_data_path: str):
    """
    è·å–ä¸­å›½å®è§‚æ æ†ç‡æ•°æ®
    """
    print('ğŸ” æ­£åœ¨è·å–ä¸­å›½å®è§‚æ æ†ç‡æ•°æ®...')
    fetch_with_cache(ak.macro_cnbs, macro_cnbs_data_path)


def fetch_macro_cn_gmjjy(macro_cn_gmjjy_data_path: str):
    """
    è·å–å›½æ°‘ç»æµè¿è¡ŒçŠ¶å†µæ•°æ®
    """
    create_path(macro_cn_gmjjy_data_path)
    print('ğŸ” æ­£åœ¨è·å–å›½æ°‘ç»æµè¿è¡ŒçŠ¶å†µæ•°æ®...')
    # è·å–ä¼ä¸šå•†å“ä»·æ ¼æŒ‡æ•°æ•°æ®
    fetch_macro_china_qyspjg(macro_cn_gmjjy_data_path + '/ä¼ä¸šå•†å“ä»·æ ¼æŒ‡æ•°æ•°æ®.csv')

    # è·å–å¤–å•†ç›´æ¥æŠ•èµ„æ•°æ®
    fetch_macro_china_fdi(macro_cn_gmjjy_data_path + '/å¤–å•†ç›´æ¥æŠ•èµ„æ•°æ®.csv')

    # è·å–LPRå“ç§æ•°æ®
    fetch_macro_china_lpr(macro_cn_gmjjy_data_path + '/LPRå“ç§æ•°æ®.csv')

    # è·å–åŸé•‡è°ƒæŸ¥å¤±ä¸šç‡
    fetch_macro_china_urban_unemployment(macro_cn_gmjjy_data_path + '/åŸé•‡è°ƒæŸ¥å¤±ä¸šç‡.csv')

    # è·å–ç¤¾ä¼šèèµ„è§„æ¨¡å¢é‡ç»Ÿè®¡
    fetch_macro_china_shrzgm(macro_cn_gmjjy_data_path + '/ç¤¾ä¼šèèµ„è§„æ¨¡å¢é‡ç»Ÿè®¡.csv')

    # è·å–ä¸­å›½GDPå¹´ç‡
    fetch_macro_china_gdp_yearly(macro_cn_gmjjy_data_path + '/ä¸­å›½GDPå¹´ç‡.csv')

    # è·å–ç‰©ä»·æ°´å¹³
    fetch_macro_china_wjsp(macro_cn_gmjjy_data_path + '/ç‰©ä»·æ°´å¹³')

def fetch_macro_china_qyspjg(macro_china_qyspjg_data_path: str):
    """
    è·å–ä¼ä¸šå•†å“ä»·æ ¼æŒ‡æ•°æ•°æ®
    """
    print('ğŸ” æ­£åœ¨è·å–ä¼ä¸šå•†å“ä»·æ ¼æŒ‡æ•°æ•°æ®...')
    fetch_with_cache(ak.macro_china_qyspjg, macro_china_qyspjg_data_path)


def fetch_macro_china_fdi(macro_china_fdi_data_path: str):
    """
    è·å–å¤–å•†ç›´æ¥æŠ•èµ„æ•°æ®
    """
    print('ğŸ” æ­£åœ¨è·å–å¤–å•†ç›´æ¥æŠ•èµ„æ•°æ®...')
    fetch_with_cache(ak.macro_china_fdi, macro_china_fdi_data_path)


def fetch_macro_china_lpr(macro_china_lpr_data_path: str):
    """
    è·å–LPRå“ç§æ•°æ®
    """
    print('ğŸ” æ­£åœ¨è·å–LPRå“ç§æ•°æ®...')
    fetch_with_cache(ak.macro_china_lpr, macro_china_lpr_data_path)


def fetch_macro_china_urban_unemployment(macro_china_urban_unemployment_data_path: str):
    """
    è·å–åŸé•‡è°ƒæŸ¥å¤±ä¸šç‡
    """
    print('ğŸ” æ­£åœ¨è·å–åŸé•‡è°ƒæŸ¥å¤±ä¸šç‡...')
    fetch_with_cache(ak.macro_china_urban_unemployment, macro_china_urban_unemployment_data_path)


def fetch_macro_china_shrzgm(macro_china_shrzgm_data_path: str):
    """
    è·å–ç¤¾ä¼šèèµ„è§„æ¨¡å¢é‡ç»Ÿè®¡
    """
    print('ğŸ” æ­£åœ¨è·å–ç¤¾ä¼šèèµ„è§„æ¨¡å¢é‡ç»Ÿè®¡...')
    fetch_with_cache(ak.macro_china_shrzgm, macro_china_shrzgm_data_path)


def fetch_macro_china_gdp_yearly(macro_china_gdp_yearly_data_path: str):
    """
    è·å–ä¸­å›½GDPå¹´ç‡
    """
    print('ğŸ” æ­£åœ¨è·å–ä¸­å›½GDPå¹´ç‡...')
    fetch_with_cache(ak.macro_china_gdp_yearly, macro_china_gdp_yearly_data_path)


def fetch_macro_china_wjsp(macro_china_wjsp_data_path: str):
    """
    è·å–ç‰©ä»·æ°´å¹³
    """
    print('ğŸ” æ­£åœ¨è·å–ç‰©ä»·æ°´å¹³...')
    create_path(macro_china_wjsp_data_path)

    # è·å–ä¸­å›½CPIå¹´ç‡æŠ¥å‘Š
    fetch_macro_china_cpi_yearly(macro_china_wjsp_data_path + '/ä¸­å›½CPIå¹´ç‡æŠ¥å‘Š.csv')

    # è·å–ä¸­å›½CPIæœˆç‡æŠ¥å‘Š
    fetch_macro_china_cpi_monthly(macro_china_wjsp_data_path + '/ä¸­å›½CPIæœˆç‡æŠ¥å‘Š.csv')

    # è·å–ä¸­å›½PPIå¹´ç‡æŠ¥å‘Š
    fetch_macro_china_ppi_yearly(macro_china_wjsp_data_path + '/ä¸­å›½PPIå¹´ç‡æŠ¥å‘Š.csv')


def fetch_macro_china_cpi_yearly(macro_china_cpi_yearly_data_path: str):
    """
    è·å–ä¸­å›½CPIå¹´ç‡æŠ¥å‘Š
    """
    print('ğŸ” æ­£åœ¨è·å–ä¸­å›½CPIå¹´ç‡æŠ¥å‘Š...')
    fetch_with_cache(ak.macro_china_cpi_yearly, macro_china_cpi_yearly_data_path)


def fetch_macro_china_cpi_monthly(macro_china_cpi_monthly_data_path: str):
    """
    è·å–ä¸­å›½CPIæœˆç‡æŠ¥å‘Š
    """
    print('ğŸ” æ­£åœ¨è·å–ä¸­å›½CPIæœˆç‡æŠ¥å‘Š...')
    fetch_with_cache(ak.macro_china_cpi_monthly, macro_china_cpi_monthly_data_path)


def fetch_macro_china_ppi_yearly(macro_china_ppi_yearly_data_path: str):
    """
    è·å–ä¸­å›½PPIå¹´ç‡æŠ¥å‘Š
    """
    print('ğŸ” æ­£åœ¨è·å–ä¸­å›½PPIå¹´ç‡æŠ¥å‘Š...')
    fetch_with_cache(ak.macro_china_ppi_yearly, macro_china_ppi_yearly_data_path)


def fetch_macro_cn_myzk(macro_cn_myzk_data_path: str):
    """
    è·å–è´¸æ˜“çŠ¶å†µæ•°æ®
    """
    create_path(macro_cn_myzk_data_path)
    print('ğŸ” æ­£åœ¨è·å–è´¸æ˜“çŠ¶å†µæ•°æ®...')

    # è·å–ä»¥ç¾å…ƒè®¡ç®—çš„å‡ºå£å¹´ç‡
    fetch_macro_china_exports_yoy(macro_cn_myzk_data_path + '/ä»¥ç¾å…ƒè®¡ç®—çš„å‡ºå£å¹´ç‡.csv')

    # è·å–ä»¥ç¾å…ƒè®¡ç®—çš„è¿›å£å¹´ç‡
    fetch_macro_china_imports_yoy(macro_cn_myzk_data_path + '/ä»¥ç¾å…ƒè®¡ç®—çš„è¿›å£å¹´ç‡.csv')

    # è·å–ä»¥ç¾å…ƒè®¡ç®—çš„è´¸æ˜“å¸ï¼ˆäº¿ç¾å…ƒï¼‰
    fetch_macro_china_trade_balance(macro_cn_myzk_data_path + '/ä»¥ç¾å…ƒè®¡ç®—çš„è´¸æ˜“å¸ï¼ˆäº¿ç¾å…ƒï¼‰.csv')


def fetch_macro_china_exports_yoy(macro_china_exports_yoy_data_path: str):
    """
    è·å–ä»¥ç¾å…ƒè®¡ç®—çš„å‡ºå£å¹´ç‡
    """
    print('ğŸ” æ­£åœ¨è·å–ä»¥ç¾å…ƒè®¡ç®—çš„å‡ºå£å¹´ç‡...')
    fetch_with_cache(ak.macro_china_exports_yoy, macro_china_exports_yoy_data_path)


def fetch_macro_china_imports_yoy(macro_china_imports_yoy_data_path: str):
    """
    è·å–ä»¥ç¾å…ƒè®¡ç®—çš„è¿›å£å¹´ç‡
    """
    print('ğŸ” æ­£åœ¨è·å–ä»¥ç¾å…ƒè®¡ç®—çš„è¿›å£å¹´ç‡...')
    fetch_with_cache(ak.macro_china_imports_yoy, macro_china_imports_yoy_data_path)


def fetch_macro_china_trade_balance(macro_china_trade_balance_data_path: str):
    """
    è·å–ä»¥ç¾å…ƒè®¡ç®—çš„è´¸æ˜“å¸ï¼ˆäº¿ç¾å…ƒï¼‰
    """
    print('ğŸ” æ­£åœ¨è·å–ä»¥ç¾å…ƒè®¡ç®—çš„è´¸æ˜“å¸ï¼ˆäº¿ç¾å…ƒï¼‰...')
    fetch_with_cache(ak.macro_china_trade_balance, macro_china_trade_balance_data_path)


def fetch_macro_cn_cyzb(macro_cn_cyzb_data_path: str):
    """
    è·å–äº§ä¸šæŒ‡æ ‡
    """
    create_path(macro_cn_cyzb_data_path)

    # è·å–å·¥ä¸šå¢åŠ å€¼å¢é•¿
    fetch_macro_china_gyzjz(macro_cn_cyzb_data_path + '/å·¥ä¸šå¢åŠ å€¼å¢é•¿.csv')

    # è·å–å®˜æ–¹åˆ¶é€ ä¸šPMI
    fetch_macro_china_pmi_yearly(macro_cn_cyzb_data_path + '/åˆ¶é€ ä¸šPMI.csv')

    # è·å–å®˜æ–¹éåˆ¶é€ ä¸šPMI
    fetch_macro_china_non_man_pmi(macro_cn_cyzb_data_path + '/éåˆ¶é€ ä¸šPMI.csv')


def fetch_macro_china_gyzjz(macro_china_gyzjz_data_path: str):
    """
    è·å–å·¥ä¸šå¢åŠ å€¼å¢é•¿
    """
    print('ğŸ” æ­£åœ¨è·å–å·¥ä¸šå¢åŠ å€¼å¢é•¿...')
    fetch_with_cache(ak.macro_china_gyzjz, macro_china_gyzjz_data_path)


def fetch_macro_china_pmi_yearly(macro_china_pmi_yearly_data_path: str):
    """
    è·å–å®˜æ–¹åˆ¶é€ ä¸šPMI
    """
    print('ğŸ” æ­£åœ¨è·å–å®˜æ–¹åˆ¶é€ ä¸šPMI...')
    fetch_with_cache(ak.macro_china_pmi_yearly, macro_china_pmi_yearly_data_path)


def fetch_macro_china_non_man_pmi(macro_china_non_man_pmi_data_path: str):
    """
    è·å–å®˜æ–¹éåˆ¶é€ ä¸šPMI
    """
    print('ğŸ” æ­£åœ¨è·å–å®˜æ–¹éåˆ¶é€ ä¸šPMI...')
    fetch_with_cache(ak.macro_china_non_man_pmi, macro_china_non_man_pmi_data_path)


def fetch_macro_cn_jrzb(macro_cn_jrzb_data_path: str):
    """
    è·å–é‡‘èæŒ‡æ ‡
    """
    create_path(macro_cn_jrzb_data_path)
    print('ğŸ” æ­£åœ¨è·å–é‡‘èæŒ‡æ ‡...')

    # è·å–å¤–æ±‡å‚¨å¤‡ï¼ˆäº¿ç¾å…ƒï¼‰
    fetch_macro_china_fx_reserves_yearly(macro_cn_jrzb_data_path + '/å¤–æ±‡å‚¨å¤‡ï¼ˆäº¿ç¾å…ƒï¼‰.csv')

    # è·å–M2è´§å¸ä¾›åº”å¹´ç‡
    fetch_macro_china_m2_yearly(macro_cn_jrzb_data_path + '/M2è´§å¸ä¾›åº”å¹´ç‡.csv')

    # è·å–ä¼ä¸šæ™¯æ°”åŠä¼ä¸šå®¶ä¿¡å¿ƒæŒ‡æ•°
    fetch_macro_china_enterprise_boom_index(macro_cn_jrzb_data_path + '/ä¼ä¸šæ™¯æ°”åŠä¼ä¸šå®¶ä¿¡å¿ƒæŒ‡æ•°.csv')

    # è·å–å±…æ°‘æ¶ˆè´¹ä»·æ ¼æŒ‡æ•°
    fetch_macro_china_cpi(macro_cn_jrzb_data_path + '/å±…æ°‘æ¶ˆè´¹ä»·æ ¼æŒ‡æ•°.csv')

    # è·å–å›½å†…ç”Ÿäº§æ€»å€¼
    fetch_macro_china_gdp(macro_cn_jrzb_data_path + '/å›½å†…ç”Ÿäº§æ€»å€¼.csv')

    # è·å–è´§å¸ä¾›åº”é‡
    fetch_macro_china_supply_of_money(macro_cn_jrzb_data_path + '/è´§å¸ä¾›åº”é‡.csv')

    # è·å–äººæ°‘å¸æ±‡ç‡ä¸­é—´ä»·
    fetch_macro_china_rmb(macro_cn_jrzb_data_path + '/äººæ°‘å¸æ±‡ç‡ä¸­é—´ä»·.csv')


def fetch_macro_china_fx_reserves_yearly(macro_china_fx_reserves_yearly_data_path: str):
    """
    è·å–å¤–æ±‡å‚¨å¤‡ï¼ˆäº¿ç¾å…ƒï¼‰
    """
    print('ğŸ” æ­£åœ¨è·å–å¤–æ±‡å‚¨å¤‡ï¼ˆäº¿ç¾å…ƒï¼‰...')
    fetch_with_cache(ak.macro_china_fx_reserves_yearly, macro_china_fx_reserves_yearly_data_path)


def fetch_macro_china_m2_yearly(macro_china_m2_yearly_data_path: str):
    """
    è·å–M2è´§å¸ä¾›åº”å¹´ç‡
    """
    print('ğŸ” æ­£åœ¨è·å–M2è´§å¸ä¾›åº”å¹´ç‡...')
    fetch_with_cache(ak.macro_china_m2_yearly, macro_china_m2_yearly_data_path)


def fetch_macro_china_enterprise_boom_index(macro_china_enterprise_boom_index_data_path: str):
    """
    è·å–ä¼ä¸šæ™¯æ°”åŠä¼ä¸šå®¶ä¿¡å¿ƒæŒ‡æ•°
    """
    print('ğŸ” æ­£åœ¨è·å–ä¼ä¸šæ™¯æ°”åŠä¼ä¸šå®¶ä¿¡å¿ƒæŒ‡æ•°...')
    fetch_with_cache(ak.macro_china_enterprise_boom_index, macro_china_enterprise_boom_index_data_path)


def fetch_macro_china_cpi(macro_china_cpi_data_path: str):
    """
    è·å–å±…æ°‘æ¶ˆè´¹ä»·æ ¼æŒ‡æ•°
    """
    print('ğŸ” æ­£åœ¨è·å–å±…æ°‘æ¶ˆè´¹ä»·æ ¼æŒ‡æ•°...')
    fetch_with_cache(ak.macro_china_cpi, macro_china_cpi_data_path)


def fetch_macro_china_gdp(macro_china_gdp_data_path: str):
    """
    è·å–å›½å†…ç”Ÿäº§æ€»å€¼
    """
    print('ğŸ” æ­£åœ¨è·å–å›½å†…ç”Ÿäº§æ€»å€¼...')
    fetch_with_cache(ak.macro_china_gdp, macro_china_gdp_data_path)


def fetch_macro_china_supply_of_money(macro_china_supply_of_money_data_path: str):
    """
    è·å–è´§å¸ä¾›åº”é‡
    """
    print('ğŸ” æ­£åœ¨è·å–è´§å¸ä¾›åº”é‡...')
    fetch_with_cache(ak.macro_china_supply_of_money, macro_china_supply_of_money_data_path)


def fetch_macro_china_rmb(macro_china_rmb_data_path: str):
    """
    è·å–äººæ°‘å¸æ±‡ç‡ä¸­é—´ä»·
    """
    print('ğŸ” æ­£åœ¨è·å–äººæ°‘å¸æ±‡ç‡ä¸­é—´ä»·...')
    fetch_with_cache(ak.macro_china_rmb, macro_china_rmb_data_path)


def fetch_macro_us(macro_us_data_path: str):
    """
    è·å–ç¾å›½å®è§‚æ•°æ®
    """
    create_path(macro_us_data_path)



def parse_diverse_date(date_str: str, date_column: str) -> Optional[datetime]:
    """
    è§£æå¤šç§æ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸²
    
    Args:
        date_str: åŸå§‹æ—¥æœŸå­—ç¬¦ä¸²
        date_column: æ—¥æœŸåˆ—çš„åç§°ï¼ˆç”¨äºåˆ¤æ–­æ ¼å¼ï¼‰
    
    Returns:
        è§£ææˆåŠŸçš„datetimeå¯¹è±¡ï¼Œè§£æå¤±è´¥è¿”å›None
    """
    if pd.isna(date_str):
        return None
    
    date_str = str(date_str).strip()
    
    try:
        # å¤„ç†æ ¼å¼: 2023-05-31
        if re.match(r'^\d{4}-\d{1,2}-\d{1,2}$', date_str):
            return datetime.strptime(date_str, '%Y-%m-%d')
        
        # å¤„ç†æ ¼å¼: 2008å¹´02æœˆä»½
        if 'æœˆä»½' in date_column and re.match(r'^\d{4}å¹´\d{1,2}æœˆä»½$', date_str):
            return datetime.strptime(date_str, '%Yå¹´%mæœˆä»½')
        
        # å¤„ç†æ ¼å¼: 1991-04-21 (TRADE_DATEåˆ—)
        if 'TRADE_DATE' in date_column and re.match(r'^\d{4}-\d{1,2}-\d{1,2}$', date_str):
            return datetime.strptime(date_str, '%Y-%m-%d')
        
        # å¤„ç†æ ¼å¼: 202011 (dateåˆ—)
        if 'date' in date_column.lower() and re.match(r'^\d{6}$', date_str):
            return datetime.strptime(date_str, '%Y%m')
        
        # å¤„ç†æ ¼å¼: 201501 (æœˆä»½åˆ—)
        if 'æœˆä»½' in date_column and re.match(r'^\d{6}$', date_str):
            return datetime.strptime(date_str, '%Y%m')
        
        # å¤„ç†æ ¼å¼: 2024å¹´ç¬¬4å­£åº¦
        if 'å­£åº¦' in date_column and re.match(r'^\d{4}å¹´ç¬¬[1-4]å­£åº¦$', date_str):
            year = int(date_str[:4])
            quarter = int(date_str[6])
            month = quarter * 3
            day = 31 if month in [3, 12] else 30
            return datetime(year, month, day)
        
        # å¤„ç†æ ¼å¼: 2024å¹´ç¬¬1-4å­£åº¦
        if 'å­£åº¦' in date_column and re.match(r'^\d{4}å¹´ç¬¬[1-4]-[1-4]å­£åº¦$', date_str):
            year = int(date_str[:4])
            return datetime(year, 12, 31)
        
        # å¤„ç†æ ¼å¼: 2025.2 (ç»Ÿè®¡æ—¶é—´åˆ—)
        if 'ç»Ÿè®¡æ—¶é—´' in date_column and re.match(r'^\d{4}\.\d{1,2}$', date_str):
            return datetime.strptime(date_str, '%Y.%m')
        
        # å¤„ç†æ ¼å¼: 2005-03 (å¹´ä»½åˆ—)
        if 'å¹´ä»½' in date_column and re.match(r'^\d{4}-\d{1,2}$', date_str):
            return datetime.strptime(date_str, '%Y-%m')

        # å¤„ç†æ ¼å¼: 2025-04-01 11:33:01
        if re.match(r'^\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2}$', date_str):
            return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
        
    except ValueError:
        print(f"âŒ æ— æ³•è§£ææ—¥æœŸå­—ç¬¦ä¸²: {date_str}")
        return None
    
    return None


def filter_csv_by_date_range(input_path: str, 
                            output_path: str,
                            start_date: str, 
                            end_date: str,
                            backup_dir: str = None):
    """
    å¤„ç†å•ä¸ªCSVæ–‡ä»¶ï¼ŒæŒ‰æ—¥æœŸèŒƒå›´ç­›é€‰æ•°æ®å¹¶ä¿å­˜ï¼ŒåŒæ—¶å¤‡ä»½åŸå§‹æ–‡ä»¶
    
    Args:
        input_path: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
        start_date: å¼€å§‹æ—¥æœŸ (æ ¼å¼: %Y%m%d) (åŒ…å«)
        end_date: ç»“æŸæ—¥æœŸ (æ ¼å¼: %Y%m%d) (åŒ…å«)
        backup_dir: å¤‡ä»½ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºNoneï¼Œåˆ™ä¸å¤‡ä»½
    """
    # è½¬æ¢æ—¥æœŸèŒƒå›´
    start_dt = datetime.strptime(start_date, '%Y%m%d')
    end_dt = datetime.strptime(end_date, '%Y%m%d').replace(hour=23, minute=59, second=59)
    
    # è¯»å–CSVæ–‡ä»¶
    df = pd.read_csv(input_path, encoding='utf-8', low_memory=False)
    
    # æŸ¥æ‰¾æ—¥æœŸåˆ—
    date_columns = [col for col in df.columns 
                   if any(keyword in col for keyword in 
                         ['æ—¥æœŸ', 'æœˆä»½', 'TRADE_DATE', 'date', 'å­£åº¦', 'ç»Ÿè®¡æ—¶é—´', 'å¹´ä»½', 'æŠ¥å‘ŠæœŸ', 'å‘å¸ƒæ—¶é—´']) 
                         and col != 'æœ€æ–°å…¬å‘Šæ—¥æœŸ']
    
    if not date_columns:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¥æœŸåˆ—ï¼Œç›´æ¥å¤åˆ¶æ–‡ä»¶åˆ°è¾“å‡ºè·¯å¾„
        shutil.copy2(input_path, output_path)
        print(f"æç¤º: æ–‡ä»¶ {os.path.basename(input_path)} æœªæ‰¾åˆ°æ—¥æœŸåˆ—ï¼Œå·²ç›´æ¥å¤åˆ¶åˆ°è¾“å‡ºè·¯å¾„")
        return
    
    # å¤‡ä»½åŸå§‹æ–‡ä»¶
    if backup_dir:
        os.makedirs(backup_dir, exist_ok=True)
        backup_path = os.path.join(backup_dir, os.path.basename(input_path))
        shutil.copy2(input_path, backup_path)
    
    # ç­›é€‰æ•°æ®
    filtered_dfs = []
    for date_col in date_columns:
        # è§£ææ—¥æœŸåˆ—
        df['parsed_date'] = df[date_col].apply(
            lambda x: parse_diverse_date(x, date_col))
        # ç­›é€‰æ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
        mask = (df['parsed_date'] >= start_dt) & (df['parsed_date'] <= end_dt)
        filtered_df = df.loc[mask].copy()
        
        if not filtered_df.empty:
            filtered_df.drop(columns=['parsed_date'], inplace=True)
            filtered_dfs.append(filtered_df)
    
    # åˆå¹¶å¹¶ä¿å­˜ç»“æœ
    if filtered_dfs:
        result_df = pd.concat(filtered_dfs).drop_duplicates()
        result_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"å¤„ç†å®Œæˆ: {os.path.basename(input_path)} -> ä¿å­˜ {len(result_df)} è¡Œæ•°æ®")
    else:
        print(f"è­¦å‘Š: æ–‡ä»¶ {os.path.basename(input_path)} åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ— æ•°æ®")


def process_all_csvs_in_directory(root_dir: str,
                                 output_dir: str,
                                 start_date: str,
                                 end_date: str,
                                 backup_dir: str = None):
    """
    æˆªå–æ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
    é€’å½’å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰CSVæ–‡ä»¶
    
    Args:
        root_dir: è¦å¤„ç†çš„æ ¹ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        start_date: å¼€å§‹æ—¥æœŸ (æ ¼å¼: %Y%m%d) åŒ…å«
        end_date: ç»“æŸæ—¥æœŸ (æ ¼å¼: %Y%m%d) åŒ…å«
        backup_dir: å¤‡ä»½ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºNoneï¼Œåˆ™ä¸å¤‡ä»½
    """
    print(f"ğŸ” å¼€å§‹å¤„ç†ç›®å½•: {root_dir}")
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # éå†æ‰€æœ‰CSVæ–‡ä»¶
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.lower().endswith('.csv'):
                input_path = os.path.join(root, file)
                # ä¿æŒåŸå§‹ç›®å½•ç»“æ„
                rel_path = os.path.relpath(root, root_dir)
                if rel_path == '.':
                    rel_path = ''
                print(f"root: {root}, rel_path: {rel_path}")
                
                output_subdir = os.path.join(output_dir, rel_path)
                os.makedirs(output_subdir, exist_ok=True)
                
                output_path = os.path.join(output_subdir, file)
                
                # å¤„ç†æ–‡ä»¶
                try:
                    filter_csv_by_date_range(
                        input_path=input_path,
                        output_path=output_path,
                        start_date=start_date,
                        end_date=end_date,
                        backup_dir=os.path.join(backup_dir, rel_path) if backup_dir else None
                    )
                except Exception as e:
                    print(f"å¤„ç†æ–‡ä»¶ {input_path} æ—¶å‡ºé”™: {str(e)}")
    print(f"âœ… å¤„ç†ç›®å½•å®Œæˆ: {root_dir}")


if __name__ == "__main__":
    process_all_csvs_in_directory(
        root_dir="data/å®è§‚æ•°æ®",
        output_dir="output_data/å®è§‚æ•°æ®",
        start_date="20220101",
        end_date="20250331"
        # backup_dir="backup_data/å®è§‚æ•°æ®"
    )

    # filter_csv_by_date_range(
    #     input_path="data/å®è§‚æ•°æ®/ä¸­å›½å®è§‚æ•°æ®/é‡‘èæŒ‡æ ‡/ä¼ä¸šæ™¯æ°”åŠä¼ä¸šå®¶ä¿¡å¿ƒæŒ‡æ•°.csv",
    #     output_path="output_data/å®è§‚æ•°æ®/ä¸­å›½å®è§‚æ•°æ®/é‡‘èæŒ‡æ ‡/ä¼ä¸šæ™¯æ°”åŠä¼ä¸šå®¶ä¿¡å¿ƒæŒ‡æ•°.csv",
    #     start_date="20210101",
    #     end_date="20250331"
    #     # backup_dir="backup_data/å®è§‚æ•°æ®/ä¸­å›½å®è§‚æ•°æ®/äº§ä¸šæŒ‡æ ‡"
    # )

